% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/spectral.R
\name{expgrad}
\alias{expgrad}
\title{Exponentiated Gradient with L2 Loss}
\usage{
expgrad(X, y, XtX = NULL, alpha = NULL, tol = 1e-07, max.its = 500)
}
\arguments{
\item{X}{The transposed feature matrix.}

\item{y}{The target vector}

\item{XtX}{Optionally a precalculated crossproduct.}

\item{alpha}{An optional initialization of the parameter.  Otherwise it starts at 1/nrow(X).}

\item{tol}{Convergence tolerance}

\item{max.its}{Maximum iterations to run irrespective of tolerance}
}
\value{
\item{par}{Optimal weights}
\item{its}{Number of iterations run}
\item{converged}{Logical indicating if it converged}
\item{entropy}{Entropy of the resulting weights}
\item{log.sse}{Log of the sum of squared error}
}
\description{
Find the optimal convex combination of features X which approximate the vector y under
and L2 loss.
}
\details{
An implementation of RecoverL2 based on David Mimno's code.  In this setting the
objective and gradient can both be kernalized leading to faster computations than
possible under the KL loss.  Specifically the computations are no longer dependent
on the size of the vocabulary making the operation essentially linear on the total
vocab size.
The matrix X'X can be passed as an argument in settings
like spectral algorithms where it is constant across multiple optimizations.
}

